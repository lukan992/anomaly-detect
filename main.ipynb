{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot, plot_ts, plot_rws, plot_error, unroll_ts\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datetime import datetime\n",
    "#импорт модели\n",
    "from model import hyperparameters\n",
    "from orion.primitives.tadgan import TadGAN\n",
    "\n",
    "from orion.primitives.tadgan import score_anomalies\n",
    "\n",
    "from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка даных с датчика и запись названия всех датчиков в список col_names для возмоности поиска аномалий со всех датчиков единоразово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m project_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "project_data = pd.read_csv('results.csv')\n",
    "col_names = []\n",
    "for col_name in project_data.columns[1:-1]:\n",
    "    col_names.append(col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование датафрейма в вид со значениями датчика и временем снятия данных с датчика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = col_names[1]\n",
    "project_data[sensor_name] = pd.to_numeric(project_data[sensor_name], errors='coerce')\n",
    "project_data = project_data[[\"timestamp\", sensor_name]]\n",
    "project_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование столбца с временем в столец с секундами, прошедшими от съёма первых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Две строки с датами\n",
    "date1_str = project_data['timestamp'][0]\n",
    "for i in range(project_data.shape[0]):\n",
    "    date2_str = project_data['timestamp'].iloc[i]\n",
    "\n",
    "    # Преобразование строк в datetime объекты\n",
    "    date1 = datetime.strptime(date1_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date2 = datetime.strptime(date2_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Перевод в UNIX-время (в секундах с 1 января 1970 года)\n",
    "    timestamp1 = int(date1.timestamp())\n",
    "    timestamp2 = int(date2.timestamp())\n",
    "\n",
    "    # Разница во времени в секундах\n",
    "    difference_in_seconds = timestamp2 - timestamp1\n",
    "    project_data['timestamp'][i] = difference_in_seconds\n",
    "\n",
    "# Вывод результата\n",
    "print(f\"Timestamp 1: {timestamp1} секунд {date1_str}\")\n",
    "print(f\"Timestamp 2: {timestamp2} секунд{date2_str}\")\n",
    "print(f\"Разница: {difference_in_seconds} секунд\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение двух массивов с данными и временем для дальнейшей обработки и создание датафрейма из этих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = project_data['timestamp'].tolist()\n",
    "b = project_data[sensor_name].tolist()\n",
    "\n",
    "data = {'timestamp': a, 'value': b}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция агрегирует значения за заданный промежуток времени, то есть собирает в массив данные с промежутком во времени равным переданному значению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"Aggregate values over given time span.\n",
    "    Args:\n",
    "        X (ndarray or pandas.DataFrame):\n",
    "            N-dimensional sequence of values.\n",
    "        interval (int):\n",
    "            Integer denoting time span to compute aggregation of.\n",
    "        time_column (int):\n",
    "            Column of X that contains time values.\n",
    "        method (str or list):\n",
    "            Optional. String describing aggregation method or list of strings describing multiple\n",
    "            aggregation methods. If not given, `mean` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray:\n",
    "            * Sequence of aggregated values, one column for each aggregation method.\n",
    "            * Sequence of index values (first index of each aggregated segment).\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    X = X.sort_values(time_column).set_index(time_column)\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    start_ts = X.index.values[0]\n",
    "    max_ts = X.index.values[-1]\n",
    "\n",
    "    values = list()\n",
    "    index = list()\n",
    "    while start_ts <= max_ts:\n",
    "        end_ts = start_ts + interval\n",
    "        subset = X.loc[start_ts:end_ts - 1]\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method\n",
    "        ]\n",
    "        values.append(np.concatenate(aggregated))\n",
    "        index.append(start_ts)\n",
    "        start_ts = end_ts\n",
    "\n",
    "    return np.asarray(values), np.asarray(index)\n",
    "\n",
    "X, index = time_segments_aggregate(df, interval=60, time_column='timestamp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем пропуски данных и масштабируем (нормализируем) данные для последующей обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X = imp.fit_transform(X)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "plot_ts(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение скользящего окна на наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
    "                             drop=None, drop_windows=False):\n",
    "    \"\"\"Create rolling window sequences out of time series data.\n",
    "    The function creates an array of input sequences and an array of target sequences by rolling\n",
    "    over the input sequence with a specified window.\n",
    "    Optionally, certain values can be dropped from the sequences.\n",
    "    Args:\n",
    "        X (ndarray):\n",
    "            N-dimensional sequence to iterate over.\n",
    "        index (ndarray):\n",
    "            Array containing the index values of X.\n",
    "        window_size (int):\n",
    "            Length of the input sequences.\n",
    "        target_size (int):\n",
    "            Length of the target sequences.\n",
    "        step_size (int):\n",
    "            Indicating the number of steps to move the window forward each round.\n",
    "        target_column (int):\n",
    "            Indicating which column of X is the target.\n",
    "        drop (ndarray or None or str or float or bool):\n",
    "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
    "            indicating which value should be dropped. If not given, `None` is used.\n",
    "        drop_windows (bool):\n",
    "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
    "            given, `False` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray, ndarray, ndarray:\n",
    "            * input sequences.\n",
    "            * target sequences.\n",
    "            * first index value of each input sequence.\n",
    "            * first index value of each target sequence.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)\n",
    "\n",
    "X, y, X_index, y_index = rolling_window_sequences(X, index, \n",
    "                                                  window_size=100, \n",
    "                                                  target_size=1, \n",
    "                                                  step_size=1,\n",
    "                                                  target_column=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод размерности данных и самих окон для наглядности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data input shape: {}\".format(X.shape))\n",
    "print(\"Training data index shape: {}\".format(X_index.shape))\n",
    "print(\"Training y shape: {}\".format(y.shape))\n",
    "print(\"Training y index shape: {}\".format(y_index.shape))\n",
    "plot_rws(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели TadGAN и установка гипепарамеров(https://arxiv.org/abs/2009.07769)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters[\"epochs\"] = 3\n",
    "hyperparameters[\"input_shape\"] = (100, 1) # based on the window size\n",
    "hyperparameters[\"optimizer\"] = \"tensorflow.keras.optimizers.Adam\"\n",
    "hyperparameters[\"learning_rate\"] = 0.0005\n",
    "hyperparameters[\"latent_dim\"] = 20\n",
    "hyperparameters[\"batch_size\"] = 64\n",
    "\n",
    "tgan = TadGAN(**hyperparameters)\n",
    "tgan.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказывание данных и просмотр получившихся окон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat, critic = tgan.predict(X)\n",
    "\n",
    "# visualize X_hat\n",
    "plot_rws(X_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод оригиналиного и сгенерированного графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = unroll_ts(X_hat)\n",
    "\n",
    "# plot the time series\n",
    "plot_ts([y, y_hat], labels=['original', 'reconstructed'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление ошибки генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.zeros(shape=y.shape)\n",
    "length = y.shape[0]\n",
    "for i in range(length):\n",
    "    error[i] = abs(y_hat[i] - y[i])\n",
    "\n",
    "# visualize the error curve\n",
    "fig = plt.figure(figsize=(30, 3))\n",
    "plt.plot(error)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графиков: показания с датчика, предсказанные данные датчика поверх снятых с датчика данных, графики вероятности аномалий(на втором графике хорошо видно совокупности аномалий, на третьем графике хорошо видно точечные аномалии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error_aggregate, true_index, true, pred = score_anomalies(X, X_hat, critic, X_index, rec_error_type=\"dtw\", comb=\"mult\")\n",
    "error_pinpoints, _, _, _ = score_anomalies(X, X_hat, critic, X_index, rec_error_type=\"dtw\", comb=\"mult\", smooth=False)\n",
    "pred = np.array(pred).mean(axis=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отображение графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error([[true, pred], error_pinpoints])\n",
    "plot_error([[true, pred], error_aggregate])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
